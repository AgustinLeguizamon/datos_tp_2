{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[{"output_type":"stream","text":"/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/contractions/english_contractions.json\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"markdown","source":"> ## Lectura de data sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extracting the number of examples of each class\nreal_tweets = df_train[df_train['target'] == 1]\nnot_real_tweets = df_train[df_train['target'] == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real_len = real_tweets.shape[0]\nnot_real_len = not_real_tweets.shape[0]\nprint(\"Cantidad de tweets reales:\", real_len)\nprint(\"Cantidad de tweets falsos:\",not_real_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bar plot of the 2 classes\nplt.bar(10,real_len,3, label=\"Real\", color='blue')\nplt.bar(15,not_real_len,3, label=\"Not\", color='red')\nplt.legend()\nplt.ylabel('Cantidad de tweets')\nplt.title('Proporcion reales/falsos')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text preprocess\n\nhttps://kavita-ganesan.com/text-preprocessing-tutorial/#.XybT-h-YU8q"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install symspellpy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importamos librerias de pre procesamiento de texto\nimport nltk\nimport re\nimport string\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm\nimport spacy\nfrom symspellpy.symspellpy import SymSpell, Verbosity\nimport pkg_resources\nimport json","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train Shape:\", df_train.shape)\nprint(\"Test Shape:\", df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Punctuation Removal"},{"metadata":{},"cell_type":"markdown","source":"Escape special characters in pattern. This is useful if you want to match an arbitrary literal string that may have regular expression metacharacters in it. For example:\n\ninput:\nprint(re.escape('htt://ww.python.org'))\n\noutput:\nhtt://ww\\.python\\.org\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#utilizamos la libreria \"re\" para limpieza de texto\n#uso funcion \n#re.sub(pattern, repl, string, count=0, flags=0)\n#elimino corchetes, links, <>, puntuaciones, saltos de linea\n#y remuevo las palabras que contienen numeros\n\n#remuevo todas las puntuaciones salvo # y @\n#no remuevo urls pq los pienso reemplazar mas adelante\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    #text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    punctuations_to_remove = string.punctuation.replace(\"#\",\"\")\n    punctuations_to_remove = punctuations_to_remove.replace(\"@\",\"\")\n    text = re.sub('[%s]' % re.escape(punctuations_to_remove), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n#Remuevo emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = \"!#$%&hola'()*+, -./:;<=>?@[\\]^_`{|}~\"\nprint(clean_text(sample))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text Normalization - Stemming/Lemmatization"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Penn Treebank Tokenizer\n#The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank.\n\ndef myStemmer(text):\n    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n    tokens = tokenizer.tokenize(text)\n    stemmer = nltk.stem.PorterStemmer()\n    stemmed_list = []\n    for token in tokens:\n        stemmed_list.append(stemmer.stem(token))\n    stemmed_text = \"\"\n    separator = ' '\n    stemmed_text = separator.join(stemmed_list)\n    return stemmed_text\n\ndef myLemmatizer(text):\n    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n    tokens = tokenizer.tokenize(text)\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    lemmatized_list = []\n    for token in tokens:\n        lemmatized_list.append(lemmatizer.lemmatize(token))\n    lemmatized_text = \"\"\n    separator = ' '\n    lemmatized_text = separator.join(lemmatized_list)\n    return lemmatized_text\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#The strip() method returns a \n#copy of the string by removing both the leading and the \n#trailing characters (based on the string argument passed).\n\ndef lemmatize(sentence):\n    nlp = spacy.load('en')\n    return (_lemmatize_text(sentence, nlp).strip())\n    \ndef _lemmatize_text(sentence, nlp):\n    sent = \"\"\n    doc = nlp(sentence)\n    for token in doc:\n        if '@' in token.text:\n            sent+=\" @MENTION\"\n        elif '#' in token.text:\n            sent+= \" #HASHTAG\"\n        else:\n            sent+=\" \"+token.lemma_\n    return sent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = \"feet cats #wolves talked\"\nprint(\"Stemming:\", myStemmer(sample))\nprint(\"Lemmatizing: \", myLemmatizer(sample))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence = \"our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\"\nlemmatize(sentence)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Simplify Punctuation and whitespace"},{"metadata":{"trusted":true},"cell_type":"code","source":"def simplify(sentence):\n    sent = _replace_urls(sentence)\n    sent = _simplify_punctuation(sent)\n    sent = _normalize_whitespace(sent)\n    return sent\n\ndef _replace_urls(text):\n    url_regex = r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n    text = re.sub(url_regex, \"<URL>\", text)\n    return text\n\ndef _simplify_punctuation(text):\n    \"\"\"\n    This function simplifies doubled or more complex punctuation. The exception is '...'.\n    \"\"\"\n    corrected = str(text)\n    corrected = re.sub(r'([!?,;])\\1+', r'\\1', corrected)\n    corrected = re.sub(r'\\.{2,}', r'...', corrected)\n    return corrected\n\ndef _normalize_whitespace(text):\n    \"\"\"\n    This function normalizes whitespaces, removing duplicates.\n    \"\"\"\n    corrected = str(text)\n    corrected = re.sub(r\"//t\",r\"\\t\", corrected)\n    corrected = re.sub(r\"( )\\1+\",r\"\\1\", corrected)\n    corrected = re.sub(r\"(\\n)\\1+\",r\"\\1\", corrected)\n    corrected = re.sub(r\"(\\r)\\1+\",r\"\\1\", corrected)\n    corrected = re.sub(r\"(\\t)\\1+\",r\"\\1\", corrected)\n    return corrected.strip(\" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = \"12312 31our Deeds are the #(%/)#asdReason of this #earthquake May ALLAH Forgive us all\"\nprint(simplify(sample))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Substitution of contractions:\nUsing a list of contractions from Wikipedia, we loop through the sentences and replace the contractions for their actual words (this benefits from happening before tokenization, since one token is broken into two). This helps in better sentence structuring later. The list can be downloaded here."},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_contractions(text, contractions):\n    \"\"\"\n    This function normalizes english contractions.\n    \"\"\"\n    new_token_list = []\n    token_list = text.split()\n    for word_pos in range(len(token_list)):\n        word = token_list[word_pos]\n        first_upper = False\n        if word[0].isupper():\n            first_upper = True\n        if word.lower() in contractions:\n            replacement = contractions[word.lower()]\n            if first_upper:\n                replacement = replacement[0].upper()+replacement[1:]\n            replacement_tokens = replacement.split()\n            if len(replacement_tokens)>1:\n                new_token_list.append(replacement_tokens[0])\n                new_token_list.append(replacement_tokens[1])\n            else:\n                new_token_list.append(replacement_tokens[0])\n        else:\n            new_token_list.append(word)\n    sentence = \" \".join(new_token_list).strip(\" \")\n    return sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"contraction_list = json.loads(open('../input/contractions/english_contractions.json', 'r').read())    \nsample = \"I'm afraid btw...\"\nprint(normalize_contractions(sample, contraction_list))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Spell Correction:\n- Now, this is a tricky one. It can (and will) cause some unwanted changes (most spell correcting dictionaries lack important contextual words, so they consider them as misspells). So you have to use it consciously. There are many ways to do it. I chose to use a module named symspellpy, which is really fast (this matters a lot!) and does the job reasonably well. Another way to do it, is to train a deep learning model to spell correction based on context, but this is another story entirely."},{"metadata":{"trusted":true},"cell_type":"code","source":"def init_spellchecker():\n    max_edit_distance_dictionary= 3\n    prefix_length = 4\n    spellchecker = SymSpell(max_edit_distance_dictionary, prefix_length)\n    dictionary_path = pkg_resources.resource_filename(\n        \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n    bigram_path = pkg_resources.resource_filename(\n        \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n    spellchecker.load_dictionary(dictionary_path, term_index=0, count_index=1)\n    spellchecker.load_bigram_dictionary(dictionary_path, term_index=0, count_index=2)\n    return spellchecker\n    \ndef spell_correction(text, spellchecker):\n    \"\"\"\n    This function does very simple spell correction normalization using pyspellchecker module. It works over a tokenized sentence and only the token representations are changed.\n    \"\"\"\n    if len(text) < 1:\n        return \"\"\n    #Spell checker config\n    max_edit_distance_lookup = 2\n    suggestion_verbosity = Verbosity.TOP # TOP, CLOSEST, ALL\n    #End of Spell checker config\n    token_list = text.split()\n    for word_pos in range(len(token_list)):\n        word = token_list[word_pos]\n        if word is None:\n            token_list[word_pos] = \"\"\n            continue\n        if not '\\n' in word and word not in string.punctuation and not is_numeric(word) and not (word.lower() in spellchecker.words.keys()):\n            suggestions = spellchecker.lookup(word.lower(), suggestion_verbosity, max_edit_distance_lookup)\n            #Checks first uppercase to conserve the case.\n            upperfirst = word[0].isupper()\n            #Checks for correction suggestions.\n            if len(suggestions) > 0:\n                correction = suggestions[0].term\n                replacement = correction\n            #We call our _reduce_exaggerations function if no suggestion is found. Maybe there are repeated chars.\n            else:\n                replacement = _reduce_exaggerations(word)\n            #Takes the case back to the word.\n            if upperfirst:\n                replacement = replacement[0].upper()+replacement[1:]\n            word = replacement\n            token_list[word_pos] = word\n    return \" \".join(token_list).strip()\n\ndef _reduce_exaggerations(text):\n    \"\"\"\n    Auxiliary function to help with exxagerated words.\n    Examples:\n        woooooords -> words\n        yaaaaaaaaaaaaaaay -> yay\n    \"\"\"\n    correction = str(text)\n    #TODO work on complexity reduction.\n    return re.sub(r'([\\w])\\1+', r'\\1', correction)\n\ndef is_numeric(text):\n    for char in text:\n        if not (char in \"0123456789\" or char in \",%.$\"):\n            return False\n    return True\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#spellchecker = init_spellchecker()\n#sample = \"aaaaaaaaah Haha South Tampa is getting flooded hah- WAIT A SECOND I LIVE IN SOUTH TAMPA WHAT AM I Going to DO WHAT AM I Going to DO FVCK #flooding\"\n#print(spell_correction(sample, spellchecker))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenization & Stop-words removal"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(words_list):\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    return tokenizer.tokenize(words_list)\n\ndef remove_stopwords(text, stop_words_list = stopwords.words('english')):\n    words = []\n    for w in text:\n        if w not in stop_words_list:\n            words.append(w)\n    return words\n\ndef combine_text(list_of_text):\n    combined_text = ' '.join(list_of_text)\n    return combined_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = \"the air feels fresh this day\"\nsample = tokenize(sample)\nprint(sample)\nprint(remove_stopwords(sample))\nprint(combine_text(remove_stopwords(sample)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_preprocessing(text, nlp, contractions, spellchecker):\n    text = simplify(text)\n    text = normalize_contractions(text, contractions)\n    text = spell_correction(text, spellchecker)\n    #la limpieza la hago al final dado \n    #que remueve todas las puntuaciones y muchas son importantes\n    #para la normalizacion del texto como (')\n    text = clean_text(text)\n    #text = myLemmatizer(text)\n    #text = myStemmer(text)\n    text = _lemmatize_text(text, nlp).strip()\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en')\nspellchecker = init_spellchecker()\ncontraction_list = json.loads(open('../input/contractions/english_contractions.json', 'r').read())    \nsample = \"12312 31our @ mark @Deeds are the #(%/) asdReason of this #earthquake May ALLAH Forgive us all https://www.kaggle.com/agustnleguizamn/tp2-preprocess/edit/run/39789151\" \nprint(text_preprocessing(sample, nlp, contraction_list, spellchecker))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = \"#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires\"\nprint(text_preprocessing(sample, nlp, contraction_list, spellchecker))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aplico funciones de preprocesamiento al data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"#pre- procesamiento\ndf_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['text'][5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en')\nspellchecker = init_spellchecker()\ncontraction_list = json.loads(open('../input/contractions/english_contractions.json', 'r').read())    \n#Train set\ndf_train['text'] = df_train['text']\\\n    .apply(lambda x: text_preprocessing(x, nlp, contraction_list, spellchecker))\n#Test set\ndf_test['text'] = df_test['text']\\\n    .apply(lambda x: text_preprocessing(x, nlp, contraction_list, spellchecker))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['text'][5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Models\n\n**Text classification**\nhttps://monkeylearn.com/text-classification/#:~:text=Text%20classification%20is%20the%20process,spam%20detection%2C%20and%20intent%20detection.\n\n**Stacking Scikit-Learn API**\nhttps://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/\n\nDefino los modelos a utilizar para entrenar\n\nThe get_stacking() function below defines the StackingClassifier model by first defining a list of tuples for the five base models, then defining the logistic regression meta-model to combine the predictions from the base models using 5-fold cross-validation."},{"metadata":{},"cell_type":"markdown","source":"https://towardsdatascience.com/handling-imbalanced-datasets-in-deep-learning-f48407a0e758\n\nDel tp1 sabemos que es mas facil identificar tweets sobre accidentes porque estos tienden a estar mejor escritos y tienen palabras/terminos en comun mientras que los tweets que no son sobre desastres pueden ser de cualquier otro tema.\nPor lo tanto buscamos que los tweets que son sobre accidentes tengan un peso menor para asi dar mas enfasis a los casos dificles de clasificar.\n\nLista de HP que no funcionaron:\n- lr: C > 0.1 or C < 0.1 \n- lr: class_weight predefinido"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB, ComplementNB\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC, OneClassSVM\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import RidgeClassifier, Perceptron, SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import model_selection\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer, Normalizer\nfrom sklearn.pipeline import make_pipeline\n\nfrom matplotlib import pyplot\n\nclass_weight={1:0.5, \n              0:0.5}\n\ndef get_models():\n    models = dict()\n    #linear_model\n    models['lr'] = LogisticRegression(max_iter=1000, class_weight='balanced', C=1e-1)\n    models['rc'] = RidgeClassifier()\n    models['sdgc'] = SGDClassifier()\n    models['knn'] = KNeighborsClassifier()\n    models['cart'] = DecisionTreeClassifier()\n    #models['svm'] = SVC()\n    models['lSvc'] = LinearSVC(max_iter = 1500)\n    models['nuSvc'] = NuSVC()\n    #models['OCSvm'] = OneClassSVM()\n    #models['gnb'] = GaussianNB()\n    models['mnb'] = MultinomialNB()\n    models['cnb'] = ComplementNB()\n    models['per'] = Perceptron()\n    return models\n\ndef get_stacking(models):\n    #define the base models\n    level_0 = list()\n    for name, model in models.items():\n        level_0.append((name,model))\n    level_1 = LogisticRegression(max_iter=1000)\n    model = StackingClassifier(estimators=level_0, \n                               final_estimator=level_1)\n    return model\n\ndef evaluate_model(model, train_vectors):\n    #cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = model_selection.cross_val_score(model, train_vectors, df_train['target'], cv=5, scoring = 'f1')\n    return scores ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparo y evaluo los distintos modelos"},{"metadata":{},"cell_type":"markdown","source":"### Feature Extraction\nUtilizo BOW y TF-IDF para extraera las features a partir del texto"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer, HashingVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature extraction - BOW\ncount_vectorizer = CountVectorizer()\ntrain_bow = count_vectorizer.fit_transform(df_train['text'])\nprint(count_vectorizer.get_feature_names())\ntest_bow = count_vectorizer.transform(df_test['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TF IDF\ntfidf_vectorizer = TfidfVectorizer()\ntrain_tfidf = tfidf_vectorizer.fit_transform(df_train['text'])\nprint(tfidf_vectorizer.get_feature_names())\ntest_tfidf = tfidf_vectorizer.transform(df_test['text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate models"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = get_models()\nresults, names = list(), list()\nprint(\"BOW\")\nfor name, model in models.items():\n    scores = evaluate_model(model, train_bow)\n    results.append(scores)\n    names.append(name)\n    print(name, scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.boxplot(results, labels=names, showmeans=True)\nplt.title('Modelos con BOW')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = get_models()\nresults_tfidf, names_tfidf = list(), list()\nprint(\"TF IDF\")\nfor name, model in models.items():\n    scores = evaluate_model(model, train_tfidf)\n    results_tfidf.append(scores)\n    names_tfidf.append(name)\n    #print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n    print(name, scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.boxplot(results_tfidf, labels=names_tfidf, showmeans=True)\nplt.title('Modelos con TF-IDF')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"#No funciona\nclf = GaussianNB()\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)   \nscores = model_selection.cross_val_score(clf, train_bow, df_train[\"target\"], cv=cv, scoring=\"f1\")\nnp.mean(scores)"},{"metadata":{},"cell_type":"markdown","source":"## Evaluo Stacking y comparo con los modelos"},{"metadata":{"trusted":true},"cell_type":"raw","source":"stacking = get_stacking(models)\nscores = evaluate_model(stacking, train_bow)\nprint(\"All models stacked with BOW\")\nscores"},{"metadata":{"trusted":true},"cell_type":"raw","source":"results.append(scores)\nnames.append(\"stacking\")\nprint('>%s %.3f (%.3f)' % (\"stacking\", np.mean(scores), np.std(scores)))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.title('Modelos y sklearn-stacking con BOW')\nplt.show()"},{"metadata":{},"cell_type":"markdown","source":"A partir de los scores de cada modelo podemos ver que la vectorizacion con BOW tiene valores \ncon menor varianza mientras que TF-IDF tiende a haber una distancia mayor entre minimos y maximos de scores\nLos 4 mejores modelos en ambos casos son LogisticRegression, MultioniamlNB, ComplementeNB y NuSVC()\n\nVuelvo a definir el modelo de stacking pero con solo estas 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_best_stacking():\n    level_0 = list()\n    level_0.append((\"lr\", LogisticRegression(max_iter = 150, class_weight='balanced', C=1e-1)))\n    level_0.append((\"cnb\", ComplementNB(alpha=1)))\n    level_0.append((\"mnb\", MultinomialNB(alpha=1)))\n    level_0.append((\"nuSvc\", NuSVC()))\n    level_1= LogisticRegression()\n    model = StackingClassifier(estimators=level_0, final_estimator=level_1)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_stacking = get_best_stacking()\n#Stacking 3 modelos + BOW\nscores = evaluate_model(best_stacking, train_bow) \nprint(scores)\n#Stacking 3 modelos + TF-IDF\nscores_tfidf = evaluate_model(best_stacking, train_tfidf)\nprint(scores_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.append(scores)\nnames.append(\"stacking\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.boxplot(results, labels=names, showmeans=True)\nplt.title('Modelos con BOW y stacking')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_tfidf.append(scores_tfidf)\nnames_tfidf.append(\"stacking\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.boxplot(results_tfidf, labels=names_tfidf, showmeans=True)\nplt.title('Modelos con TFIDF y stacking')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hiper-parametros feature extraction\nProbamos cambiar los hiperparametros de los algoritmos de feature extraction\n\nLista de hiperparametros que no funcionaron:\n- strip_accents\n- stop_words\n- ngram_range\n- max_df\n- min_df\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"hp_count_vectorizer = CountVectorizer()\ntrain_bow_hp = hp_count_vectorizer.fit_transform(df_train['text'])\ntest_bow_hp = hp_count_vectorizer.transform(df_test['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hp_tfidf_vectorizer = TfidfVectorizer()\ntrain_tfidf_hp = hp_tfidf_vectorizer.fit_transform(df_train['text'])\ntest_tfidf_hp = hp_tfidf_vectorizer.transform(df_test['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"raw","source":"print(evaluate_model(best_stacking, train_bow_hp))\nprint(evaluate_model(best_stacking, train_tfidf_hp))"},{"metadata":{},"cell_type":"markdown","source":"### Feature preprocess - Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline_models, pipeline_names= list(), list()\n\nmodels_pip = dict()\nmodels_pip['svc'] = SVC()\n#models_pip['ocs'] = OneClassSVM()\n#models_pip['gnb'] = GaussianNB()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"print(\"default\")\nfor name, model in models_pip.items():\n    scores = evaluate_model(model, train_bow)\n    print(name, scores)"},{"metadata":{},"cell_type":"markdown","source":"Agrego al diccionario los mismos modelos pero como un pipeline\ncon transformaciones de los features"},{"metadata":{"trusted":true},"cell_type":"code","source":"models_pip['svc_p'] = make_pipeline(StandardScaler(with_mean=False),SVC())\n#models_pip['gnb_p'] = make_pipeline(StandardScaler(with_mean=False), GaussianNB())\n#models_pip['ocs_p'] = make_pipeline(StandardScaler(with_mean=False), OneClassSVM())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"raw","source":"print(\"Pipeline\")\nfor name, model in models_pip.items():\n    scores = evaluate_model(model, train_bow)\n    print(name, scores)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#sacado de Fork of TP2,\nclf = XGBClassifier(objective = 'binary:logistic', \n                    random_state=42, \n                    seed=2, \n                    colsample_bytree=0.5, \n                    subsample=0.7,\n                    learning_rate=0.1,\n                    n_estimators=300\n                    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(evaluate_model(clf, train_bow))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(train_bow,df_train['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Fitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_stacking.fit(train_bow, df_train['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submission(submission_file_path, model ,test_vectors):\n    sample_submission = pd.read_csv(submission_file_path)\n    sample_submission['target'] = model.predict(test_vectors)\n    sample_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file_path = \"../input/nlp-getting-started/sample_submission.csv\"\nsubmission_test_vectors=test_bow_hp\n#Eligo el metodo que me dio mayor puntaje, en este caso fue el Naive Bayes con TF_IDF\nsubmission(submission_file_path,best_stacking, submission_test_vectors)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Verifico submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub = pd.read_csv(\"./submission.csv\")\ndf_test_copy = df_test.copy(deep=True)\ndf_test_copy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_copy['target'] = df_sub['target']\ndf_test_copy.head(30)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}