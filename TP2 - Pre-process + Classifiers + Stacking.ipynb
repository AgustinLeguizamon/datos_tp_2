{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/contractions/english_contractions.json\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"markdown","source":"> ## Lectura de data sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extracting the number of examples of each class\nreal_tweets = df_train[df_train['target'] == 1]\nnot_real_tweets = df_train[df_train['target'] == 0]","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real_len = real_tweets.shape[0]\nnot_real_len = not_real_tweets.shape[0]\nprint(\"Cantidad de tweets reales:\", real_len)\nprint(\"Cantidad de tweets falsos:\",not_real_len)","execution_count":5,"outputs":[{"output_type":"stream","text":"Cantidad de tweets reales: 3271\nCantidad de tweets falsos: 4342\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bar plot of the 2 classes\nplt.bar(10,real_len,3, label=\"Real\", color='blue')\nplt.bar(15,not_real_len,3, label=\"Not\", color='red')\nplt.legend()\nplt.ylabel('Cantidad de tweets')\nplt.title('Proporcion reales/falsos')\nplt.show()","execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"## Text preprocess\n\nhttps://kavita-ganesan.com/text-preprocessing-tutorial/#.XybT-h-YU8q"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install symspellpy","execution_count":7,"outputs":[{"output_type":"stream","text":"Collecting symspellpy\n  Downloading symspellpy-6.5.2-py3-none-any.whl (2.6 MB)\n\u001b[K     |████████████████████████████████| 2.6 MB 573 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.13.1 in /opt/conda/lib/python3.7/site-packages (from symspellpy) (1.18.5)\nInstalling collected packages: symspellpy\nSuccessfully installed symspellpy-6.5.2\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importamos librerias de pre procesamiento de texto\nimport nltk\nimport re\nimport string\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm\nimport spacy\nfrom symspellpy.symspellpy import SymSpell, Verbosity\nimport pkg_resources\nimport json","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train Shape:\", df_train.shape)\nprint(\"Test Shape:\", df_test.shape)","execution_count":9,"outputs":[{"output_type":"stream","text":"Train Shape: (7613, 5)\nTest Shape: (3263, 4)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Punctuation Removal"},{"metadata":{"trusted":true},"cell_type":"code","source":"#utilizamos la libreria \"re\" para limpieza de texto\n#uso funcion \n#re.sub(pattern, repl, string, count=0, flags=0)\n#elimino corchetes, links, <>, puntuaciones, saltos de linea\n#y remuevo las palabras que contienen numeros\n\n#remuevo todas las puntuaciones salvo # y @\n#no remuevo urls pq los pienso reemplazar mas adelante\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    #text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    punctuations_to_remove = string.punctuation.replace(\"#\",\"\")\n    punctuations_to_remove = punctuations_to_remove.replace(\"@\",\"\")\n    text = re.sub('[%s]' % re.escape(punctuations_to_remove), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n#Remuevo emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = \"!#$%&hola'()*+, -./:;<=>?@[\\]^_`{|}~\"\nprint(clean_text(sample))","execution_count":11,"outputs":[{"output_type":"stream","text":"#hola @\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Text Normalization - Stemming/Lemmatization"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Penn Treebank Tokenizer\n#The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank.\n\ndef myStemmer(text):\n    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n    tokens = tokenizer.tokenize(text)\n    stemmer = nltk.stem.PorterStemmer()\n    stemmed_list = []\n    for token in tokens:\n        stemmed_list.append(stemmer.stem(token))\n    stemmed_text = \"\"\n    separator = ' '\n    stemmed_text = separator.join(stemmed_list)\n    return stemmed_text\n\ndef myLemmatizer(text):\n    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n    tokens = tokenizer.tokenize(text)\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    lemmatized_list = []\n    for token in tokens:\n        lemmatized_list.append(lemmatizer.lemmatize(token))\n    lemmatized_text = \"\"\n    separator = ' '\n    lemmatized_text = separator.join(lemmatized_list)\n    return lemmatized_text\n    ","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#The strip() method returns a \n#copy of the string by removing both the leading and the \n#trailing characters (based on the string argument passed).\n\ndef lemmatize(sentence):\n    nlp = spacy.load('en')\n    return (_lemmatize_text(sentence, nlp).strip())\n    \ndef _lemmatize_text(sentence, nlp):\n    sent = \"\"\n    doc = nlp(sentence)\n    for token in doc:\n        if '@' in token.text:\n            sent+=\" @MENTION\"\n        elif '#' in token.text:\n            sent+= \" #HASHTAG\"\n        else:\n            sent+=\" \"+token.lemma_\n    return sent","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Simplify Punctuation and whitespace"},{"metadata":{"trusted":true},"cell_type":"code","source":"def simplify(sentence):\n    sent = _replace_urls(sentence)\n    sent = _simplify_punctuation(sent)\n    sent = _normalize_whitespace(sent)\n    return sent\n\ndef _replace_urls(text):\n    url_regex = r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n    text = re.sub(url_regex, \"<URL>\", text)\n    return text\n\ndef _simplify_punctuation(text):\n    corrected = str(text)\n    corrected = re.sub(r'([!?,;])\\1+', r'\\1', corrected)\n    corrected = re.sub(r'\\.{2,}', r'...', corrected)\n    return corrected\n\ndef _normalize_whitespace(text):\n    corrected = str(text)\n    corrected = re.sub(r\"//t\",r\"\\t\", corrected)\n    corrected = re.sub(r\"( )\\1+\",r\"\\1\", corrected)\n    corrected = re.sub(r\"(\\n)\\1+\",r\"\\1\", corrected)\n    corrected = re.sub(r\"(\\r)\\1+\",r\"\\1\", corrected)\n    corrected = re.sub(r\"(\\t)\\1+\",r\"\\1\", corrected)\n    return corrected.strip(\" \")","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Substitution of contractions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_contractions(text, contractions):\n    \"\"\"\n    This function normalizes english contractions.\n    \"\"\"\n    new_token_list = []\n    token_list = text.split()\n    for word_pos in range(len(token_list)):\n        word = token_list[word_pos]\n        first_upper = False\n        if word[0].isupper():\n            first_upper = True\n        if word.lower() in contractions:\n            replacement = contractions[word.lower()]\n            if first_upper:\n                replacement = replacement[0].upper()+replacement[1:]\n            replacement_tokens = replacement.split()\n            if len(replacement_tokens)>1:\n                new_token_list.append(replacement_tokens[0])\n                new_token_list.append(replacement_tokens[1])\n            else:\n                new_token_list.append(replacement_tokens[0])\n        else:\n            new_token_list.append(word)\n    sentence = \" \".join(new_token_list).strip(\" \")\n    return sentence","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"contraction_list = json.loads(open('../input/contractions/english_contractions.json', 'r').read())    \nsample = \"I'm afraid btw...\"\nprint(normalize_contractions(sample, contraction_list))","execution_count":16,"outputs":[{"output_type":"stream","text":"I am afraid btw...\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Spell Correction:\nEsta correcion ortografica genera algunos cambios inseperados dado que no se tiene en cuenta el contexto, esto es comun en los \"spell correcting dictionaries\" como el modulo symspellpy. También se podria entrenar un modelo \"deep learning\" para correcion ortografica basado en el contexto."},{"metadata":{"trusted":true},"cell_type":"code","source":"def init_spellchecker():\n    max_edit_distance_dictionary= 3\n    prefix_length = 4\n    spellchecker = SymSpell(max_edit_distance_dictionary, prefix_length)\n    dictionary_path = pkg_resources.resource_filename(\n        \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n    bigram_path = pkg_resources.resource_filename(\n        \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n    spellchecker.load_dictionary(dictionary_path, term_index=0, count_index=1)\n    spellchecker.load_bigram_dictionary(dictionary_path, term_index=0, count_index=2)\n    return spellchecker\n    \ndef spell_correction(text, spellchecker):\n    \"\"\"\n    This function does very simple spell correction normalization using pyspellchecker module. It works over a tokenized sentence and only the token representations are changed.\n    \"\"\"\n    if len(text) < 1:\n        return \"\"\n    #Spell checker config\n    max_edit_distance_lookup = 2\n    suggestion_verbosity = Verbosity.TOP # TOP, CLOSEST, ALL\n    #End of Spell checker config\n    token_list = text.split()\n    for word_pos in range(len(token_list)):\n        word = token_list[word_pos]\n        if word is None:\n            token_list[word_pos] = \"\"\n            continue\n        if not '\\n' in word and word not in string.punctuation and not is_numeric(word) and not (word.lower() in spellchecker.words.keys()):\n            suggestions = spellchecker.lookup(word.lower(), suggestion_verbosity, max_edit_distance_lookup)\n            #Checks first uppercase to conserve the case.\n            upperfirst = word[0].isupper()\n            #Checks for correction suggestions.\n            if len(suggestions) > 0:\n                correction = suggestions[0].term\n                replacement = correction\n            #We call our _reduce_exaggerations function if no suggestion is found. Maybe there are repeated chars.\n            else:\n                replacement = _reduce_exaggerations(word)\n            #Takes the case back to the word.\n            if upperfirst:\n                replacement = replacement[0].upper()+replacement[1:]\n            word = replacement\n            token_list[word_pos] = word\n    return \" \".join(token_list).strip()\n\ndef _reduce_exaggerations(text):\n    correction = str(text)\n    return re.sub(r'([\\w])\\1+', r'\\1', correction)\n\ndef is_numeric(text):\n    for char in text:\n        if not (char in \"0123456789\" or char in \",%.$\"):\n            return False\n    return True\n","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenization & Stop-words removal"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(words_list):\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    return tokenizer.tokenize(words_list)\n\ndef remove_stopwords(text, stop_words_list = stopwords.words('english')):\n    words = []\n    for w in text:\n        if w not in stop_words_list:\n            words.append(w)\n    return words\n\ndef combine_text(list_of_text):\n    combined_text = ' '.join(list_of_text)\n    return combined_text","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_preprocessing(text, nlp, contractions, spellchecker):\n    text = simplify(text)\n    text = normalize_contractions(text, contractions)\n    text = spell_correction(text, spellchecker)\n    #la limpieza la hago al final dado \n    #que remueve todas las puntuaciones y muchas son importantes\n    #para la normalizacion del texto como (')\n    text = clean_text(text)\n    #text = myLemmatizer(text)\n    #text = myStemmer(text)\n    text = _lemmatize_text(text, nlp).strip()\n    return text","execution_count":19,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aplico funciones de preprocesamiento al data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"#pre- procesamiento\ndf_train.head(10)","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  \n5       1  \n6       1  \n7       1  \n8       1  \n9       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#flood #disaster Heavy rain causes flash flood...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>13</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I'm on top of the hill and I can see a fire in...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>14</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>There's an emergency evacuation happening now ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>15</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I'm afraid that the tornado is coming to our a...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['text'][5]","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"'#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en')\nspellchecker = init_spellchecker()\ncontraction_list = json.loads(open('../input/contractions/english_contractions.json', 'r').read())    \n#Train set\ndf_train['text'] = df_train['text']\\\n    .apply(lambda x: text_preprocessing(x, nlp, contraction_list, spellchecker))\n#Test set\ndf_test['text'] = df_test['text']\\\n    .apply(lambda x: text_preprocessing(x, nlp, contraction_list, spellchecker))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['text'][5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Models\n\n**Text classification**\nhttps://monkeylearn.com/text-classification/#:~:text=Text%20classification%20is%20the%20process,spam%20detection%2C%20and%20intent%20detection.\n\n**Stacking Scikit-Learn API**\nhttps://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/\n\nDefino los modelos a utilizar para entrenar\n\nThe get_stacking() function below defines the StackingClassifier model by first defining a list of tuples for the five base models, then defining the logistic regression meta-model to combine the predictions from the base models using 5-fold cross-validation."},{"metadata":{},"cell_type":"markdown","source":"https://towardsdatascience.com/handling-imbalanced-datasets-in-deep-learning-f48407a0e758\n\nHipotesis sobre pesos de las clases:\nDel tp1 sabemos que es mas facil identificar tweets hechos no reales porque estos tienden a ser cortos, estar mal escritos y carecen de palabras claves mientras que los tweets que son sobre desastres pueden suelen estar mejor escritos y utilizan terminos relacionados a accidentes y palabras claves.\nPor lo tanto buscamos que los tweets que son sobre accidentes tengan un peso menor para asi dar mas enfasis a los casos dificiles de clasificar.\n\nLista de HP que no funcionaron:\n- lr: C > 0.1 or C < 0.1 \n- lr: class_weight predefinido"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB, ComplementNB\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC, OneClassSVM\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import RidgeClassifier, Perceptron, SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import model_selection\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer, Normalizer\nfrom sklearn.pipeline import make_pipeline\n\nfrom matplotlib import pyplot\n\n#Al final el paramtro 'balanced' termino dando mejores resultados que predefinir el peso de las clases\nclass_weight={1:0.5, \n              0:0.5}\n\ndef get_models():\n    models = dict()\n    #linear_model\n    models['lr'] = LogisticRegression(max_iter=1000, class_weight='balanced', C=1e-1)\n    models['rc'] = RidgeClassifier()\n    models['sdgc'] = SGDClassifier()\n    models['knn'] = KNeighborsClassifier()\n    models['cart'] = DecisionTreeClassifier()\n    #models['svm'] = SVC()\n    models['lSvc'] = LinearSVC(max_iter = 1500)\n    models['nuSvc'] = NuSVC()\n    #models['OCSvm'] = OneClassSVM()\n    #models['gnb'] = GaussianNB()\n    models['mnb'] = MultinomialNB()\n    models['cnb'] = ComplementNB()\n    models['per'] = Perceptron()\n    return models\n\ndef get_stacking(models):\n    #define the base models\n    level_0 = list()\n    for name, model in models.items():\n        level_0.append((name,model))\n    level_1 = LogisticRegression(max_iter=1000)\n    model = StackingClassifier(estimators=level_0, \n                               final_estimator=level_1)\n    return model\n\ndef evaluate_model(model, train_vectors):\n    #cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = model_selection.cross_val_score(model, train_vectors, df_train['target'], cv=5, scoring = 'f1')\n    return scores ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparo y evaluo los distintos modelos"},{"metadata":{},"cell_type":"markdown","source":"### Feature Extraction\nUtilizo BOW y TF-IDF para extraera las features a partir del texto"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature extraction - BOW\ncount_vectorizer = CountVectorizer()\ntrain_bow = count_vectorizer.fit_transform(df_train['text'])\n#print(count_vectorizer.get_feature_names())\ntest_bow = count_vectorizer.transform(df_test['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TF IDF\ntfidf_vectorizer = TfidfVectorizer()\ntrain_tfidf = tfidf_vectorizer.fit_transform(df_train['text'])\n#print(tfidf_vectorizer.get_feature_names())\ntest_tfidf = tfidf_vectorizer.transform(df_test['text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate models"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = get_models()\nresults, names = list(), list()\nprint(\"BOW\")\nfor name, model in models.items():\n    scores = evaluate_model(model, train_bow)\n    results.append(scores)\n    names.append(name)\n    print(name, scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.boxplot(results, labels=names, showmeans=True)\nplt.title('Modelos con BOW')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = get_models()\nresults_tfidf, names_tfidf = list(), list()\nprint(\"TF IDF\")\nfor name, model in models.items():\n    scores = evaluate_model(model, train_tfidf)\n    results_tfidf.append(scores)\n    names_tfidf.append(name)\n    #print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n    print(name, scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.boxplot(results_tfidf, labels=names_tfidf, showmeans=True)\nplt.title('Modelos con TF-IDF')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"#No funciona\nclf = GaussianNB()\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)   \nscores = model_selection.cross_val_score(clf, train_bow, df_train[\"target\"], cv=cv, scoring=\"f1\")\nnp.mean(scores)"},{"metadata":{},"cell_type":"markdown","source":"## Evaluo Stacking y comparo con los modelos"},{"metadata":{"trusted":true},"cell_type":"raw","source":"stacking = get_stacking(models)\nscores = evaluate_model(stacking, train_bow)\nprint(\"All models stacked with BOW\")\nscores"},{"metadata":{"trusted":true},"cell_type":"raw","source":"results.append(scores)\nnames.append(\"stacking\")\nprint('>%s %.3f (%.3f)' % (\"stacking\", np.mean(scores), np.std(scores)))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.title('Modelos y sklearn-stacking con BOW')\nplt.show()"},{"metadata":{},"cell_type":"markdown","source":"A partir de los scores de cada modelo podemos ver que la vectorizacion con BOW tiene valores \ncon menor varianza mientras que TF-IDF tiende a haber una distancia mayor entre minimos y maximos de scores\nLos 4 mejores modelos en ambos casos son LogisticRegression, MultioniamlNB, ComplementeNB y NuSVC()\n\nVuelvo a definir el modelo de stacking pero con solo estas 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_best_stacking():\n    level_0 = list()\n    level_0.append((\"lr\", LogisticRegression(max_iter = 150, class_weight='balanced', C=1e-1)))\n    level_0.append((\"cnb\", ComplementNB(alpha=1)))\n    level_0.append((\"mnb\", MultinomialNB(alpha=1)))\n    level_0.append((\"nuSvc\", NuSVC()))\n    level_1= LogisticRegression()\n    model = StackingClassifier(estimators=level_0, final_estimator=level_1)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_stacking = get_best_stacking()\n#Stacking 3 modelos + BOW\nscores = evaluate_model(best_stacking, train_bow) \nprint(scores)\n#Stacking 3 modelos + TF-IDF\nscores_tfidf = evaluate_model(best_stacking, train_tfidf)\nprint(scores_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.append(scores)\nnames.append(\"stacking\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.boxplot(results, labels=names, showmeans=True)\nplt.title('Modelos con BOW y stacking')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_tfidf.append(scores_tfidf)\nnames_tfidf.append(\"stacking\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.boxplot(results_tfidf, labels=names_tfidf, showmeans=True)\nplt.title('Modelos con TFIDF y stacking')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hiper-parametros feature extraction\nProbamos cambiar los hiperparametros de los algoritmos de feature extraction\n\nLista de hiperparametros que no funcionaron:\n- strip_accents\n- stop_words\n- ngram_range\n- max_df\n- min_df\n\n"},{"metadata":{"trusted":true},"cell_type":"raw","source":"hp_count_vectorizer = CountVectorizer()\ntrain_bow_hp = hp_count_vectorizer.fit_transform(df_train['text'])\ntest_bow_hp = hp_count_vectorizer.transform(df_test['text'])"},{"metadata":{"trusted":true},"cell_type":"raw","source":"hp_tfidf_vectorizer = TfidfVectorizer()\ntrain_tfidf_hp = hp_tfidf_vectorizer.fit_transform(df_train['text'])\ntest_tfidf_hp = hp_tfidf_vectorizer.transform(df_test['text'])"},{"metadata":{"trusted":true},"cell_type":"raw","source":"print(evaluate_model(best_stacking, train_bow_hp))\nprint(evaluate_model(best_stacking, train_tfidf_hp))"},{"metadata":{},"cell_type":"markdown","source":"### Feature preprocess - Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline_models, pipeline_names= list(), list()\n\nmodels_pip = dict()\nmodels_pip['svc'] = SVC()\n#models_pip['ocs'] = OneClassSVM()\n#models_pip['gnb'] = GaussianNB()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"print(\"default\")\nfor name, model in models_pip.items():\n    scores = evaluate_model(model, train_bow)\n    print(name, scores)"},{"metadata":{},"cell_type":"markdown","source":"Agrego al diccionario los mismos modelos pero como un pipeline\ncon transformaciones de los features"},{"metadata":{"trusted":true},"cell_type":"code","source":"models_pip['svc_p'] = make_pipeline(StandardScaler(with_mean=False),SVC())\n#models_pip['gnb_p'] = make_pipeline(StandardScaler(with_mean=False), GaussianNB())\n#models_pip['ocs_p'] = make_pipeline(StandardScaler(with_mean=False), OneClassSVM())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"raw","source":"print(\"Pipeline\")\nfor name, model in models_pip.items():\n    scores = evaluate_model(model, train_bow)\n    print(name, scores)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#sacado de Fork of TP2,\nclf = XGBClassifier(objective = 'binary:logistic', \n                    random_state=42, \n                    seed=2, \n                    colsample_bytree=0.5, \n                    subsample=0.7,\n                    learning_rate=0.1,\n                    n_estimators=300\n                    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(evaluate_model(clf, train_bow))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(train_bow,df_train['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Fitting\nUtilizo el mejor modelo que es el Stack con los algoritmos de sklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_stacking.fit(train_bow, df_train['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submission(submission_file_path, model ,test_vectors):\n    sample_submission = pd.read_csv(submission_file_path)\n    sample_submission['target'] = model.predict(test_vectors)\n    sample_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file_path = \"../input/nlp-getting-started/sample_submission.csv\"\nsubmission_test_vectors=test_bow\n#Eligo el metodo que me dio mayor puntaje, en este caso fue el Naive Bayes con TF_IDF\nsubmission(submission_file_path,best_stacking, submission_test_vectors)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Verifico submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub = pd.read_csv(\"./submission.csv\")\ndf_test_copy = df_test.copy(deep=True)\ndf_test_copy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_copy['target'] = df_sub['target']\ndf_test_copy.head(20)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}