{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Lectura de data sets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text preprocess","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install symspellpy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importamos librerias de pre procesamiento de texto\nimport nltk\nimport re\nimport string\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm\nimport spacy\nfrom symspellpy.symspellpy import SymSpell, Verbosity\nimport pkg_resources\nimport json","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#utilizamos la libreria \"re\" para limpieza de texto\n#uso funcion \n#re.sub(pattern, repl, string, count=0, flags=0)\n#elimino corchetes, links, <>, puntuaciones, saltos de linea\n#y remuevo las palabras que contienen numeros\n\n#remuevo todas las puntuaciones salvo # y @\n#no remuevo urls pq los pienso reemplazar mas adelante\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('<.*?>+', '', text)\n    punctuations_to_remove = string.punctuation.replace(\"#\",\"\")\n    punctuations_to_remove = punctuations_to_remove.replace(\"@\",\"\")\n    text = re.sub('[%s]' % re.escape(punctuations_to_remove), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n#Remuevo emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _lemmatize_text(sentence, nlp):\n    sent = \"\"\n    doc = nlp(sentence)\n    for token in doc:\n        if '@' in token.text:\n            sent+=\" @MENTION\"\n        elif '#' in token.text:\n            sent+= \" #HASHTAG\"\n        else:\n            sent+=\" \"+token.lemma_\n    return sent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def simplify(sentence):\n    sent = _replace_urls(sentence)\n    sent = _simplify_punctuation(sent)\n    sent = _normalize_whitespace(sent)\n    return sent\n\ndef _replace_urls(text):\n    url_regex = r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n    text = re.sub(url_regex, \"<URL>\", text)\n    return text\n\ndef _simplify_punctuation(text):\n    \"\"\"\n    This function simplifies doubled or more complex punctuation. The exception is '...'.\n    \"\"\"\n    corrected = str(text)\n    corrected = re.sub(r'([!?,;])\\1+', r'\\1', corrected)\n    corrected = re.sub(r'\\.{2,}', r'...', corrected)\n    return corrected\n\ndef _normalize_whitespace(text):\n    \"\"\"\n    This function normalizes whitespaces, removing duplicates.\n    \"\"\"\n    corrected = str(text)\n    corrected = re.sub(r\"//t\",r\"\\t\", corrected)\n    corrected = re.sub(r\"( )\\1+\",r\"\\1\", corrected)\n    corrected = re.sub(r\"(\\n)\\1+\",r\"\\1\", corrected)\n    corrected = re.sub(r\"(\\r)\\1+\",r\"\\1\", corrected)\n    corrected = re.sub(r\"(\\t)\\1+\",r\"\\1\", corrected)\n    return corrected.strip(\" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_contractions(text, contractions):\n    \"\"\"\n    This function normalizes english contractions.\n    \"\"\"\n    new_token_list = []\n    token_list = text.split()\n    for word_pos in range(len(token_list)):\n        word = token_list[word_pos]\n        first_upper = False\n        if word[0].isupper():\n            first_upper = True\n        if word.lower() in contractions:\n            replacement = contractions[word.lower()]\n            if first_upper:\n                replacement = replacement[0].upper()+replacement[1:]\n            replacement_tokens = replacement.split()\n            if len(replacement_tokens)>1:\n                new_token_list.append(replacement_tokens[0])\n                new_token_list.append(replacement_tokens[1])\n            else:\n                new_token_list.append(replacement_tokens[0])\n        else:\n            new_token_list.append(word)\n    sentence = \" \".join(new_token_list).strip(\" \")\n    return sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def init_spellchecker():\n    max_edit_distance_dictionary= 3\n    prefix_length = 4\n    spellchecker = SymSpell(max_edit_distance_dictionary, prefix_length)\n    dictionary_path = pkg_resources.resource_filename(\n        \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n    bigram_path = pkg_resources.resource_filename(\n        \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n    spellchecker.load_dictionary(dictionary_path, term_index=0, count_index=1)\n    spellchecker.load_bigram_dictionary(dictionary_path, term_index=0, count_index=2)\n    return spellchecker\n    \ndef spell_correction(text, spellchecker):\n    \"\"\"\n    This function does very simple spell correction normalization using pyspellchecker module. It works over a tokenized sentence and only the token representations are changed.\n    \"\"\"\n    if len(text) < 1:\n        return \"\"\n    #Spell checker config\n    max_edit_distance_lookup = 2\n    suggestion_verbosity = Verbosity.TOP # TOP, CLOSEST, ALL\n    #End of Spell checker config\n    token_list = text.split()\n    for word_pos in range(len(token_list)):\n        word = token_list[word_pos]\n        if word is None:\n            token_list[word_pos] = \"\"\n            continue\n        if not '\\n' in word and word not in string.punctuation and not is_numeric(word) and not (word.lower() in spellchecker.words.keys()):\n            suggestions = spellchecker.lookup(word.lower(), suggestion_verbosity, max_edit_distance_lookup)\n            #Checks first uppercase to conserve the case.\n            upperfirst = word[0].isupper()\n            #Checks for correction suggestions.\n            if len(suggestions) > 0:\n                correction = suggestions[0].term\n                replacement = correction\n            #We call our _reduce_exaggerations function if no suggestion is found. Maybe there are repeated chars.\n            else:\n                replacement = _reduce_exaggerations(word)\n            #Takes the case back to the word.\n            if upperfirst:\n                replacement = replacement[0].upper()+replacement[1:]\n            word = replacement\n            token_list[word_pos] = word\n    return \" \".join(token_list).strip()\n\ndef _reduce_exaggerations(text):\n    \"\"\"\n    Auxiliary function to help with exxagerated words.\n    Examples:\n        woooooords -> words\n        yaaaaaaaaaaaaaaay -> yay\n    \"\"\"\n    correction = str(text)\n    #TODO work on complexity reduction.\n    return re.sub(r'([\\w])\\1+', r'\\1', correction)\n\ndef is_numeric(text):\n    for char in text:\n        if not (char in \"0123456789\" or char in \",%.$\"):\n            return False\n    return True\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_preprocessing(text, nlp, contractions, spellchecker):\n    text = simplify(text)\n    text = normalize_contractions(text, contractions)\n    text = spell_correction(text, spellchecker)\n    #la limpieza la hago al final dado \n    #que remueve todas las puntuaciones y muchas son importantes\n    #para la normalizacion del texto como (')\n    text = clean_text(text)\n    text = _lemmatize_text(text, nlp).strip()\n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aplico funciones de preprocesamiento al data set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#pre- procesamiento\ndf_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en')\nspellchecker = init_spellchecker()\ncontraction_list = json.loads(open('../input/contractions/english_contractions.json', 'r').read())    \n#Train set\ndf_train['text'] = df_train['text']\\\n    .apply(lambda x: text_preprocessing(x, nlp, contraction_list, spellchecker))\n#Test set\ndf_test['text'] = df_test['text']\\\n    .apply(lambda x: text_preprocessing(x, nlp, contraction_list, spellchecker))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#post- procesamiento\ndf_train.head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GloVe Embeddings\nCargo el diccionario de embeddings con arrays de coeficientes que corresponde a cada palabra","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Importo librerias \nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\nfrom keras.layers import Embedding\nfrom keras.initializers import Constant\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, SpatialDropout1D\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = {}\nf = open('/kaggle/input/glove6b/glove.6B.100d.txt')\nfor line in f:\n    values = line.split(' ')\n    word = values[0] ## The first entry is the word\n    coefs = np.asarray(values[1:], dtype='float32') ## These are the vectors representing the embedding for the word\n    embeddings_index[word] = coefs\nf.close()\n\nprint('GloVe data loaded')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenizacion con keras","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_NUM_WORDS = 25000\nMAX_SEQUENCE_LENGTH = 50\n\ntokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(df_train['text'])\nsequences = tokenizer.texts_to_sequences(df_train['text'])\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\ndata = pad_sequences(sequences, maxlen = MAX_SEQUENCE_LENGTH)\n\nprint('Shape of data tensor:', data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_DIM = embeddings_index.get('a').shape[0]\nnum_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n\n#Inicializo la matriz con todos ceros\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word,i in word_index.items():\n    if i > MAX_NUM_WORDS:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cargo la embedding_matrix en un Embedding layer. Seteo trainable=False \npara prevenir que los pesos sean actualizados durante el entrenamiento","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_layer = Embedding(num_words,\n                            EMBEDDING_DIM,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n#Al setear trainable false, en summary aparecen como not-trainable\nmodel.add(embedding_layer)\nmodel.add(Conv1D(32, 3, activation='relu'))\nmodel.add(Conv1D(32, 3, activation='relu'))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(Conv1D(32, 3, activation='relu'))\nmodel.add(Conv1D(32, 3, activation='relu'))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmy_adam = Adam(learning_rate=5e-4)\n\nmodel.summary()\nmodel.compile(loss='binary_crossentropy', optimizer=my_adam, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fit train data\nprint(data.shape)\nmodel.fit(data, df_train['target'], validation_split=0.2,batch_size=32, epochs = 4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Preparo el test set\ntest_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\ntest_tokenizer.fit_on_texts(df_test['text'])\ntest_sequences = test_tokenizer.texts_to_sequences(df_test['text'])\ntest_data = pad_sequences(test_sequences, maxlen = MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(test_data)\npred = np.around(pred)\npred = pred.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['target'] = pred\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Verifico submission.csv","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub = pd.read_csv(\"./submission.csv\")\ndf_test_copy = df_test.copy(deep=True)\ndf_test_copy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Copio el target de la prediccion al data set de test\ndf_test_copy['target'] = df_sub['target']\ndf_test_copy.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}