{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Lectura de data sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text preprocess"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install symspellpy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importamos librerias de pre procesamiento de texto\nimport nltk\nimport re\nimport string\nfrom nltk.corpus import stopwords\n\nimport matplotlib as plt\n\nfrom tqdm import tqdm\nimport spacy\nfrom symspellpy.symspellpy import SymSpell, Verbosity\nimport pkg_resources\nimport json","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#utilizamos la libreria \"re\" para limpieza de texto\n#uso funcion \n#re.sub(pattern, repl, string, count=0, flags=0)\n#elimino corchetes, links, <>, puntuaciones, saltos de linea\n#y remuevo las palabras que contienen numeros\n\n#remuevo todas las puntuaciones salvo # y @\n#no remuevo urls pq los pienso reemplazar mas adelante\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('<.*?>+', '', text)\n    punctuations_to_remove = string.punctuation.replace(\"#\",\"\")\n    punctuations_to_remove = punctuations_to_remove.replace(\"@\",\"\")\n    text = re.sub('[%s]' % re.escape(punctuations_to_remove), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n#Remuevo emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def myStemmer(text):\n    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n    tokens = tokenizer.tokenize(text)\n    stemmer = nltk.stem.PorterStemmer()\n    stemmed_list = []\n    for token in tokens:\n        stemmed_list.append(stemmer.stem(token))\n    stemmed_text = \"\"\n    separator = ' '\n    stemmed_text = separator.join(stemmed_list)\n    return stemmed_text\n\ndef myLemmatizer(text):\n    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n    tokens = tokenizer.tokenize(text)\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    lemmatized_list = []\n    for token in tokens:\n        lemmatized_list.append(lemmatizer.lemmatize(token))\n    lemmatized_text = \"\"\n    separator = ' '\n    lemmatized_text = separator.join(lemmatized_list)\n    return lemmatized_text\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Separa la oracion en tokens y los lemmatiza, si son mentions @ o hasthtags # simplemente reemplaza estos simbolos\n#por @MENTION y #HASHTAG segun corresponda\ndef _lemmatize_text(sentence, nlp):\n    sent = \"\"\n    doc = nlp(sentence)\n    for token in doc:\n        if '@' in token.text:\n            sent+=\" @MENTION\"\n        elif '#' in token.text:\n            sent+= \" #HASHTAG\"\n        else:\n            sent+=\" \"+token.lemma_\n    return sent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def simplify(sentence):\n    sent = _replace_urls(sentence)\n    sent = _simplify_punctuation(sent)\n    sent = _normalize_whitespace(sent)\n    return sent\n\n#Reemplaza todos los urls por \"url\"\ndef _replace_urls(text):\n    url_regex = r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n    text = re.sub(url_regex, \"<URL>\", text)\n    return text\n\n#Elimina puntuaciones duplicadas\ndef _simplify_punctuation(text):\n    corrected = str(text)\n    corrected = re.sub(r'([!?,;])\\1+', r'\\1', corrected)\n    corrected = re.sub(r'\\.{2,}', r'...', corrected)\n    return corrected\n\n#remueve los espacios duplicados, de manera que haya solo 1 espacio entre palabras\ndef _normalize_whitespace(text):\n    corrected = str(text)\n    corrected = re.sub(r\"//t\",r\"\\t\", corrected)\n    corrected = re.sub(r\"( )\\1+\",r\"\\1\", corrected)\n    corrected = re.sub(r\"(\\n)\\1+\",r\"\\1\", corrected)\n    corrected = re.sub(r\"(\\r)\\1+\",r\"\\1\", corrected)\n    corrected = re.sub(r\"(\\t)\\1+\",r\"\\1\", corrected)\n    return corrected.strip(\" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_contractions(text, contractions):\n    \"\"\"\n    This function normalizes english contractions.\n    \"\"\"\n    new_token_list = []\n    token_list = text.split()\n    for word_pos in range(len(token_list)):\n        word = token_list[word_pos]\n        first_upper = False\n        if word[0].isupper():\n            first_upper = True\n        if word.lower() in contractions:\n            replacement = contractions[word.lower()]\n            if first_upper:\n                replacement = replacement[0].upper()+replacement[1:]\n            replacement_tokens = replacement.split()\n            if len(replacement_tokens)>1:\n                new_token_list.append(replacement_tokens[0])\n                new_token_list.append(replacement_tokens[1])\n            else:\n                new_token_list.append(replacement_tokens[0])\n        else:\n            new_token_list.append(word)\n    sentence = \" \".join(new_token_list).strip(\" \")\n    return sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def init_spellchecker():\n    max_edit_distance_dictionary= 3\n    prefix_length = 4\n    spellchecker = SymSpell(max_edit_distance_dictionary, prefix_length)\n    dictionary_path = pkg_resources.resource_filename(\n        \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n    bigram_path = pkg_resources.resource_filename(\n        \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n    spellchecker.load_dictionary(dictionary_path, term_index=0, count_index=1)\n    spellchecker.load_bigram_dictionary(dictionary_path, term_index=0, count_index=2)\n    return spellchecker\n\n#Usando modulo pyspellchecker se hace una correcion ortografica simple\n#Trabaja sobre texto tokenizado y solo la representacion de los tokens son alteradas.\ndef spell_correction(text, spellchecker):\n    if len(text) < 1:\n        return \"\"\n    #Spell checker config\n    max_edit_distance_lookup = 2\n    suggestion_verbosity = Verbosity.TOP # TOP, CLOSEST, ALL\n    #End of Spell checker config\n    token_list = text.split()\n    for word_pos in range(len(token_list)):\n        word = token_list[word_pos]\n        if word is None:\n            token_list[word_pos] = \"\"\n            continue\n        if not '\\n' in word and word not in string.punctuation and not is_numeric(word) and not (word.lower() in spellchecker.words.keys()):\n            suggestions = spellchecker.lookup(word.lower(), suggestion_verbosity, max_edit_distance_lookup)\n            #Checks first uppercase to conserve the case.\n            upperfirst = word[0].isupper()\n            #Checks for correction suggestions.\n            if len(suggestions) > 0:\n                correction = suggestions[0].term\n                replacement = correction\n            #We call our _reduce_exaggerations function if no suggestion is found. Maybe there are repeated chars.\n            else:\n                replacement = _reduce_exaggerations(word)\n            #Takes the case back to the word.\n            if upperfirst:\n                replacement = replacement[0].upper()+replacement[1:]\n            word = replacement\n            token_list[word_pos] = word\n    return \" \".join(token_list).strip()\n\n#Elimina las letras que estan de mas en una palabra\n#e.g yeeeeeeeeeeey -> yey\ndef _reduce_exaggerations(text):\n\n    correction = str(text)\n    #TODO work on complexity reduction.\n    return re.sub(r'([\\w])\\1+', r'\\1', correction)\n\ndef is_numeric(text):\n    for char in text:\n        if not (char in \"0123456789\" or char in \",%.$\"):\n            return False\n    return True\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_preprocessing(text, nlp, contractions, spellchecker):\n    text = simplify(text)\n    text = normalize_contractions(text, contractions)\n    text = spell_correction(text, spellchecker)\n    #la limpieza la hago al final dado \n    #que remueve todas las puntuaciones y muchas son importantes\n    #para la normalizacion del texto como (')\n    text = clean_text(text)\n    text = myStemmer(text)\n    #text = myLemmatizer(text)\n    #text = _lemmatize_text(text, nlp).strip()\n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Agrego mas features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# word_count\ndf_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\ndf_test['word_count'] = df_test['text'].apply(lambda x: len(str(x).split()))\n\n# unique_word_count\ndf_train['unique_word_count'] = df_train['text'].apply(lambda x: len(set(str(x).split())))\ndf_test['unique_word_count'] = df_test['text'].apply(lambda x: len(set(str(x).split())))\n\n# stop_word_count\ndf_train['stop_word_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words('english')]))\ndf_test['stop_word_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words('english')]))\n\n# url_count\ndf_train['url_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\ndf_test['url_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n\n# mean_word_length\ndf_train['mean_word_length'] = df_train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ndf_test['mean_word_length'] = df_test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n# char_count\ndf_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\ndf_test['char_count'] = df_test['text'].apply(lambda x: len(str(x)))\n\n# punctuation_count\ndf_train['punctuation_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ndf_test['punctuation_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n# hashtag_count\ndf_train['hashtag_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\ndf_test['hashtag_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n\n# mention_count\ndf_train['mention_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\ndf_test['mention_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aplico funciones de preprocesamiento al data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"#pre- procesamiento\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['text'][5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en')\nspellchecker = init_spellchecker()\ncontraction_list = json.loads(open('../input/contractions/english_contractions.json', 'r').read())    \n#Train set\ndf_train['text'] = df_train['text']\\\n    .apply(lambda x: text_preprocessing(x, nlp, contraction_list, spellchecker))\n#Test set\ndf_test['text'] = df_test['text']\\\n    .apply(lambda x: text_preprocessing(x, nlp, contraction_list, spellchecker))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['text'][5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#post- procesamiento\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (Opcional) Agrego los features pero despues de la limpieza"},{"metadata":{},"cell_type":"raw","source":"# word_count\ndf_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\ndf_test['word_count'] = df_test['text'].apply(lambda x: len(str(x).split()))\n\n# unique_word_count\ndf_train['unique_word_count'] = df_train['text'].apply(lambda x: len(set(str(x).split())))\ndf_test['unique_word_count'] = df_test['text'].apply(lambda x: len(set(str(x).split())))\n\n# stop_word_count\ndf_train['stop_word_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words('english')]))\ndf_test['stop_word_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words('english')]))\n\n# url_count\ndf_train['url_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w == 'urls']))\ndf_test['url_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w == 'urls']))\n\n# mean_word_length\ndf_train['mean_word_length'] = df_train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ndf_test['mean_word_length'] = df_test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n# char_count\ndf_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\ndf_test['char_count'] = df_test['text'].apply(lambda x: len(str(x)))\n\n# punctuation_count\n#df_train['punctuation_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n#df_test['punctuation_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n# hashtag_count\ndf_train['hashtag_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\ndf_test['hashtag_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n\n# mention_count\ndf_train['mention_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\ndf_test['mention_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))"},{"metadata":{},"cell_type":"raw","source":"# word_count\ndf_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\ndf_test['word_count'] = df_test['text'].apply(lambda x: len(str(x).split()))"},{"metadata":{},"cell_type":"markdown","source":"## Features extraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.pipeline import Pipeline, FeatureUnion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature extraction - BOW\ncount_vectorizer = CountVectorizer(ngram_range=(1,2))\ntrain_bow = count_vectorizer.fit_transform(df_train['text'])\n#print(count_vectorizer.get_feature_names())\ntest_bow = count_vectorizer.transform(df_test['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TF IDF\ntfidf_vectorizer = TfidfVectorizer()\ntrain_tfidf = tfidf_vectorizer.fit_transform(df_train['text'])\n#print(tfidf_vectorizer.get_feature_names())\ntest_tfidf = tfidf_vectorizer.transform(df_test['text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import DMatrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clf = XGBClassifier(random_state=42, seed=2, colsample_bytree=0.6, subsample=0.7)\nclf = XGBClassifier(objective = 'binary:logistic', \n                    random_state=42, \n                    seed=2, \n                    colsample_bytree=0.5, \n                    subsample=0.7,\n                    learning_rate=0.1,\n                    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Crear Pipeline de features\nDado que tenemos uan combinacion de features numericos y de texto, uso funciones \"helpers\" que me devuelven la columna del texto o la columna del numero. Esto me permite alimentar el algoritmo con dos tipos de features distintos. Son parte del pipeline, se inicializan con el nombre de la columna de la data a ingresar. Tienen dos metodos, fit que devuelve la clase misma y transform retorna la columna con la data que corresponde al nombre de la columna"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nclass TextSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transformer to select a single column from the data frame to perform additional transformations on\n    Use on text columns in the data\n    \"\"\"\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, X, y=None, *parg, **kwarg):\n        return self\n\n    def transform(self, X):\n        # returns the input as a string\n        return X[self.key]\n\nclass NumberSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transformer to select a single column from the data frame to perform additional transformations on\n    Use on numeric columns in the data\n    \"\"\"\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        # returns the input as a dataframe\n        return X[[self.key]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = Pipeline([\n    ('selector', TextSelector(key='text')),\n    ('vectorizer', count_vectorizer)])\n\nword_count = Pipeline([\n    ('selector',NumberSelector(key='word_count'))\n])\n\nunique_word_count = Pipeline([\n    ('selector',NumberSelector(key='unique_word_count'))\n])\n\nstop_word_count = Pipeline([\n    ('selector',NumberSelector(key='unique_word_count'))\n])\n\nurl_count = Pipeline([\n    ('selector',NumberSelector(key='url_count'))\n])\n\nmean_word_length = Pipeline([\n    ('selector',NumberSelector(key='mean_word_length'))\n])\n\nchar_count = Pipeline([\n    ('selector',NumberSelector(key='char_count'))\n])\n\npunctuation_count = Pipeline([\n    ('selector',NumberSelector(key='punctuation_count'))\n])\n\nhashtag_count = Pipeline([\n    ('selector',NumberSelector(key='hashtag_count'))\n])\n\nmention_count = Pipeline([\n    ('selector',NumberSelector(key='mention_count'))\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = FeatureUnion([('text', text),\n                        #('word_count', word_count),\n                        ('unique_word_count', unique_word_count),\n                        ('stop_word_count', stop_word_count),\n                        #('url_count', url_count),\n                        ('mean_word_length', mean_word_length),\n                        #('char_count', char_count),\n                        #('punctuation_count', punctuation_count),\n                        #('hashtag_count', hashtag_count),\n                        ('mention_count', mention_count)\n                        ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('features', features),\n                ('clf', clf)\n                ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe.named_steps['clf'].get_params()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split train y validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_features = ['text', \n                    #'word_count', \n                    'unique_word_count',\n                    'stop_word_count', \n                    #'url_count',\n                    'mean_word_length',\n                    #'char_count', \n                    #'punctuation_count', \n                    #'hashtag_count',\n                    'mention_count'\n                    ]\ntarget = 'target'\n\nX_train, X_test, y_train, y_test = train_test_split(df_train[combined_features], \n                                                    df_train[target], \n                                                    test_size=0.33, random_state=42, \n                                                    #stratify=df_train[target]\n                                                     )\n#print(X_train)\n#print(y_train)\n#print(X_test)\n#print(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tuneo hiperparametros\nUtilizando un grid search busco los mejores parametros para mi modelo, en este caso solo busco la cantidad de arboles n_estimators\nagregar mas parametros incrementa considerablemente el tiempo de busqueda"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n     'clf__n_estimators': [50,100,300],\n  #  'clf__colsample_bytree': [0.6,0.8,1]\n#    'clf__subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1]\n}\n\n# grid search cross validation instantiation\ngrid_search = GridSearchCV(estimator = pipe, param_grid = param_grid, \n                          cv = 5, n_jobs = 1, verbose = 0, return_train_score=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#hyperparameter fitting\ngrid_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.cv_results_['mean_train_score']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.cv_results_['mean_test_score']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediccion train y validacion\n#### score validacion (acertados para 2513 de validacion)\n\nTodos tienen text\n- tf-idf: 1939\n- tf-idf + word_count: 1961\n- tf_idf + todo: 1945\n- bow: 1991\n- bow + ngram(1,2) : 1992\n- bow(ngram(1,2)) + word_count : 1991\n- bow(ngram(1,2)) + word_count + unique_word_count : 1982\n- bow(ngram(1,2)) + unique_word_count : 1996\n- bow(ngram(1,2)) + unique_word_count + stop_words_count : 2004\n- bow(ngram(1,2)) + unique_word_count + stop_words_count + url_count : 2000\n- bow(ngram(1,2)) + unique_word_count + stop_words_count + mean_word_length : 2005\n- bow(ngram(1,2)) + unique_word_count + stop_words_count + mean_word_length + char_count : 1994\n- bow(ngram(1,2)) + unique_word_count + stop_words_count + mean_word_length + punctuation_count : 2005\n- bow(ngram(1,2)) + unique_word_count + stop_words_count + mean_word_length + hashtag_count: 1996\n- bow(ngram(1,2)) + unique_word_count + stop_words_count + mean_word_length + mention_count: 2006"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_test = grid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test = clf_test.predict(X_test)\n#pred_test = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Hago un df con las prediciones y los y_test para compararlos\ndic_compare_pred_test = {'pred_test':pred_test, 'y_test':y_test.values}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_compare_pred_test = pd.DataFrame(data=dic_compare_pred_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_compare_pred_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_compare_pred_test['correct'] = (df_compare_pred_test['pred_test'] == df_compare_pred_test['y_test'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_compare_pred_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_compare_pred_test['correct'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hiperparametros\nSolo texto como feature\nLos n_estimators no alteran el puntaje pq los defino con el gridSearchCV best_params\n- random_state=42, seed=2, colsample_bytree=0.2, subsample=0.7: 1959\n- random_state=42, seed=2, colsample_bytree=0.2, subsample=0.2: 1833\n- random_state=42, seed=2, colsample_bytree=0.2, subsample=0.7, n_estimators=1000 : 1959\n-  random_state=42, seed=2, colsample_bytree=0.2, subsample=0.7, n_estimators=1000, learning_rate=0.1 : 1973\n- random_state=42, seed=2, colsample_bytree=0.5, subsample=0.7, n_estimators=1000, learning_rate=0.1 : **1982**\n- random_state=42, seed=2, colsample_bytree=0.5, subsample=0.7, n_estimators=1000, learning_rate=0.5 : 1893\n- random_state=42, seed=2, colsample_bytree=0.5, subsample=0.7, n_estimators=1000, learning_rate=0.1, gamma=0.2 : 1982\n- random_state=42, seed=2, colsample_bytree=0.5, subsample=0.7, n_estimators=1000, learning_rate=0.1, gamma=0.2, alpha=10 : 1836\n- random_state=42, seed=2, colsample_bytree=0.5, subsample=0.7, n_estimators=1000, learning_rate=0.1, gamma=0.2, alpha=100 : 1699\n- random_state=42, seed=2, colsample_bytree=0.5, subsample=0.7, n_estimators=1000, learning_rate=0.1, gamma=0.2, alpha=1 : 1953\n- random_state=42, seed=2, colsample_bytree=0.5, subsample=0.7, n_estimators=1000, learning_rate=0.1, gamma=0.2, alpha=0.5 : 1961\n- random_state=42, seed=2, colsample_bytree=0.5, subsample=0.7, n_estimators=1000, learning_rate=0.1, gamma=0.8 : 1978\n- random_state=42, seed=2, colsample_bytree=0.5, subsample=0.7, n_estimators=1000, learning_rate=0.1, gamma=10 : 1892\n- random_state=42, seed=2, colsample_bytree=0.5, subsample=0.7, n_estimators=1000, learning_rate=0.1, gamma=1 : 1977\n- random_state=42, seed=2, colsample_bytree=0.5, subsample=0.7, learning_rate=0.1, max_depth=15 : 1948\n-  random_state=42, seed=2, colsample_bytree=0.5, subsample=0.7, learning_rate=0.1, max_depth=5 : 1971\n- random_state=42, seed=2, colsample_bytree=0.5, subsample=0.7, learning_rate=0.1, max_depth=50 : 1929\n- random_state=42, seed=2, colsample_bytree=0.5, subsample=0.7, learning_rate=0.1, min_child_weight=3 : 1920\n- random_state=42, seed=2, colsample_bytree=0.5, subsample=0.7, learning_rate=0.1, min_child_weight=10 : 1843\n- random_state=42, seed=2, colsample_bytree=0.5, subsample=0.7, learning_rate=0.02 : 1899\n- random_state=42, seed=2, colsample_bytree=0.5, subsample=0.7, learning_rate=0.2 : 1971\n\n#### bow(n_gram(1,2) \n- random_state=42, seed=2, colsample_bytree=0.5, subsample=0.7, learning_rate=0.01: 1852\n- colsample_bytree=0.5, subsample=0.7, learning_rate=0.1 : 1992\n- random_state=42, seed=2, colsample_bytree=0.5, subsample=0.7, learning_rate=0.1, max_depth=1 : 1902\n-  random_state=42, seed=2, colsample_bytree=0.5, subsample=0.7, learning_rate=0.1 : **2006**\n                 \n                   "},{"metadata":{},"cell_type":"markdown","source":"## Predict\n\nDespues de validarlo ahora hacemos la prediccion con el test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"#obtengo los features del test set\ndata = df_test[combined_features]\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = clf_test.predict(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['target'] = pred\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Verifico submission.csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub = pd.read_csv(\"./submission.csv\")\ndf_test_copy = df_test.copy(deep=True)\ndf_test_copy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Copio el target de la prediccion al data set de test\ndf_test_copy['target'] = df_sub['target']\ndf_test_copy.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}